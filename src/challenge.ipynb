{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción General: q1_memory(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q1_memory.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para analizar la actividad diaria y determinar el usuario más activo por fecha, optimizando el uso de memoria.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La función recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `SMALL_CHUNK_SIZE` para definir el tamaño de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaño sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplean tres componentes principales:\n",
    "  - **JsonChunkReader:** Se encarga de leer el archivo JSON por partes, facilitando el manejo de grandes volúmenes de datos.\n",
    "  - **TweetAggregator:** Realiza la agregación de datos, como el conteo de tweets por fecha y por usuario.\n",
    "  - **TweetAnalyzer:** Utiliza el lector y el agregador para analizar la información y determinar, entre otros resultados, cuál es el usuario más activo en las fechas de mayor actividad.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene la fecha y el nombre del usuario más activo en esa fecha. Además, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **Obtención de Argumentos y Configuración Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicación mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolección de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecución del código.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la función `q1_memory(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario más activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **Finalización del Perfilado y Visualización de Estadísticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadísticas obtenidas durante la ejecución.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepción, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo información completa de la excepción.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la función q1_memory(file_path)\n",
    "1. **Inicialización de Componentes:**\n",
    "   - Se crea una instancia de `JsonChunkReader` pasando la ruta del archivo (file_path) y el tamaño de fragmento (`SMALL_CHUNK_SIZE`).\n",
    "   - Se instancia un objeto `TweetAggregator` que se encargará de agrupar y contar los datos.\n",
    "   - Se crea un objeto `TweetAnalyzer` al que se le suministran el lector y el agregador, estableciendo así la conexión entre la lectura y el análisis de los datos.\n",
    "\n",
    "2. **Análisis y Agregación de Datos:**\n",
    "   - Se llama al método `analyze()` del objeto `TweetAnalyzer`. Este método procesa los datos del archivo en fragmentos, contando los tweets por fecha y determinando el usuario más activo de cada una de las 10 fechas con mayor actividad utilizando métodos de la clase `TweetAggregator`.\n",
    "\n",
    "3. **Visualización y Retorno de Resultados:**\n",
    "   - Los resultados, que consisten en una lista de tuplas `(fecha, usuario)`, se imprimen utilizando `pprint` para facilitar su visualización.\n",
    "   - Finalmente, la función retorna estos resultados.\n",
    "\n",
    "## Resumen\n",
    "Este código sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeños fragmentos.\n",
    "- **Facilitar el mantenimiento:** Separando la lógica en componentes específicos (lector, agregador y analizador).\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla contiene la fecha y el nombre del usuario más activo en esa fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
      " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
      " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
      " (datetime.date(2021, 2, 16), 'jot__b'),\n",
      " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
      " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
      " (datetime.date(2021, 2, 15), 'jot__b'),\n",
      " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
      " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
      " (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    17     96.1 MiB     96.1 MiB           1   @profile\n",
      "    18                                         def q1_memory(file_path: str) -> List[Tuple[date, str]]:\n",
      "    19                                             \"\"\"\n",
      "    20                                             CALL: q1_memory(file_path: str)\n",
      "    21                                             DESCRIPTION: Processes a JSON file to extract the top user for each of the top 10 dates (Focus on optimizing memory).\n",
      "    22                                             RESULT: List[Tuple[date, str]]\n",
      "    23                                             \"\"\"\n",
      "    24     96.1 MiB      0.0 MiB           1       reader = JsonChunkReader(file_path, SMALL_CHUNK_SIZE)\n",
      "    25     96.1 MiB      0.0 MiB           1       aggregator = TweetAggregator()\n",
      "    26     96.1 MiB      0.0 MiB           1       analyzer = TweetAnalyzer(reader, aggregator)\n",
      "    27    205.6 MiB    109.4 MiB           1       results = analyzer.analyze()\n",
      "    28                                         \n",
      "    29    205.6 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    30    205.6 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 10.249084 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q1_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción General: q1_time(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q1_time.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para analizar la actividad diaria y determinar el usuario más activo por fecha, optimizando el tiempo.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La función recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `MEDIUM_CHUNK_SIZE` para definir el tamaño de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaño sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplea el componente **TweetThreadAnalyzer** que se encarga de leer y procesar el archivo JSON en fragmentos de tamaño definido por `MEDIUM_CHUNK_SIZE` en fomra concurrente utilizando la librería `concurrent.futures`.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene la fecha y el nombre del usuario más activo en esa fecha. Además, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **Obtención de Argumentos y Configuración Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicación mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolección de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecución del código.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la función `q1_time(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario más activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **Finalización del Perfilado y Visualización de Estadísticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadísticas obtenidas durante la ejecución.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepción, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo información completa de la excepción.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la función q1_time(file_path)\n",
    "1. **Determinación del número de workers:**\n",
    "   - `num_workers = multiprocessing.cpu_count()`\n",
    "   - Se obtiene el número de núcleos de CPU disponibles en el sistema, lo que permite definir cuántos hilos se utilizarán para el procesamiento en paralelo.\n",
    "\n",
    "2. **Instanciación del Analizador:**\n",
    "   - `analyzer = TweetThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)`\n",
    "   - Se crea una instancia de la clase `TweetThreadAnalyzer`, que se encarga de leer y procesar el archivo JSON en fragmentos de tamaño definido por `MEDIUM_CHUNK_SIZE`.\n",
    "   - El parámetro `file_path` indica la ruta del archivo a procesar.\n",
    "   - `num_workers` se utiliza para configurar el número de hilos que se ejecutarán en paralelo, optimizando así el procesamiento.\n",
    "\n",
    "3. **Análisis y Agregación de Datos:**\n",
    "   - `results = analyzer.analyze()`\n",
    "   - Se invoca el método `analyze()` del objeto `analyzer`, el cual procesa los datos en paralelo y agrega los resultados. Estos resultados incluyen la identificación de las fechas con mayor actividad y el usuario más activo en cada una de ellas.\n",
    "\n",
    "4. **Visualización y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la función retorna los resultados obtenidos.\n",
    "\n",
    "## Resumen\n",
    "Este código sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeños fragmentos.\n",
    "- **Acelerar el procesamiento:** utiliza concurrencia para acelerar el procesamiento de grandes volúmenes de datos en un archivo JSON.\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla contiene la fecha y el nombre del usuario más activo en esa fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
      " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
      " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
      " (datetime.date(2021, 2, 16), 'jot__b'),\n",
      " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
      " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
      " (datetime.date(2021, 2, 15), 'jot__b'),\n",
      " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
      " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
      " (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    117.6 MiB    117.6 MiB           1   @profile    \n",
      "    16                                         def q1_time(file_path: str) -> List[Tuple[date, str]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q1_time(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file concurrently to extract the top user for each of the top 10 dates (Focus on optimizing time).\n",
      "    20                                             RESULT: List[Tuple[date, str]]\n",
      "    21                                             \"\"\"\n",
      "    22    117.6 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    23    117.6 MiB      0.0 MiB           1       analyzer = TweetThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)\n",
      "    24    169.9 MiB     52.4 MiB           1       results = analyzer.analyze()\n",
      "    25                                         \n",
      "    26    169.7 MiB     -0.2 MiB           1       pprint(results, sort_dicts=False)\n",
      "    27    169.7 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 9.930427 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q1_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción General: q2_memory(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q2_memory.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON y extraer los 10 emojis más utilizados, optimizando el uso de memoria.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La función recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `SMALL_CHUNK_SIZE` para definir el tamaño de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaño sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplean tres componentes principales:\n",
    "  - **JsonChunkReader:** Se encarga de leer el archivo JSON por partes, facilitando el manejo de grandes volúmenes de datos.\n",
    "  - **EmojiAggregator:** Realiza la agregación de datos, como el conteo emojis más utilizados.\n",
    "  - **EmojiAnalyzer:** Utiliza el lector y el agregador para analizar la información y determinar los 10 emojis más utilizados con su respectivo conteo.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el emoji y su respectivo conteo. Además, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **Obtención de Argumentos y Configuración Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicación mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolección de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecución del código.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la función `q2_memory(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario más activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **Finalización del Perfilado y Visualización de Estadísticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadísticas obtenidas durante la ejecución.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepción, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo información completa de la excepción.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la función q2_memory(file_path)\n",
    "1. **Inicialización de Componentes:**\n",
    "   - **`JsonChunkReader(file_path, SMALL_CHUNK_SIZE)`**  \n",
    "     Se crea una instancia de `JsonChunkReader` que se encarga de leer el archivo JSON en fragmentos (chunks) pequeños, definidos por `SMALL_CHUNK_SIZE`. Esto permite manejar archivos de gran tamaño sin sobrecargar la memoria. \n",
    "   - **`EmojiAggregator()`**  \n",
    "     Se instancia un objeto encargado de agrupar y contar los emojis extraídos del archivo. El agregador recopila las ocurrencias de cada emoji a medida que se procesan los fragmentos.\n",
    "   - **`EmojiAnalyzer(reader, aggregator)`**  \n",
    "     Se crea el analizador que utiliza el lector y el agregador para procesar el archivo. El analizador coordina la lectura y el conteo de emojis para finalmente identificar los 10 emojis más utilizados.\n",
    "\n",
    "2. **Análisis y Agregación de Datos:**\n",
    "   - **`results = analyzer.analyze()`**  \n",
    "     Se llama al método `analyze()` del analizador. Este método procesa cada fragmento del archivo, utiliza el agregador para contar las ocurrencias de los emojis y finalmente devuelve una lista de tuplas. Cada tupla contiene un emoji (tipo `str`) y el número de veces que aparece (tipo `int`) utilizando métodos de la clase EmojiAggregator.\n",
    "\n",
    "3. **Visualización y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la función retorna los resultados obtenidos, una lista de tuplas `(emoji, conteo)`.\n",
    "\n",
    "## Resumen\n",
    "Este código sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeños fragmentos.\n",
    "- **Facilitar el mantenimiento:** Separando la lógica en componentes específicos (lector, agregador y analizador).\n",
    "- **Obtener resultados claros:** Retornando una lista de los emojis más populares y sus respectivas cantidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 7286),\n",
      " ('😂', 3072),\n",
      " ('🚜', 2972),\n",
      " ('✊', 2411),\n",
      " ('🌾', 2363),\n",
      " ('🏻', 2080),\n",
      " ('❤', 1779),\n",
      " ('🤣', 1668),\n",
      " ('🏽', 1218),\n",
      " ('👇', 1108)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    132.3 MiB    132.3 MiB           1   @profile\n",
      "    16                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q2_memory(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file to extract the top 10 most used emojis (Focus on optimizing memory).\n",
      "    20                                             RESULT: List[Tuple[str, int]\n",
      "    21                                             \"\"\"\n",
      "    22    132.3 MiB      0.0 MiB           1       reader = JsonChunkReader(file_path, SMALL_CHUNK_SIZE)\n",
      "    23    132.3 MiB      0.0 MiB           1       aggregator = EmojiAggregator()\n",
      "    24    132.3 MiB      0.0 MiB           1       analyzer = EmojiAnalyzer(reader, aggregator)\n",
      "    25    281.7 MiB    149.5 MiB           1       results = analyzer.analyze()\n",
      "    26                                             \n",
      "    27    281.2 MiB     -0.5 MiB           1       pprint(results, sort_dicts=False)\n",
      "    28    281.2 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 19.155698 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q2_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción General: q2_time(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q2_time.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON y extraer los 10 emojis más utilizados, optimizando el tiempo.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La función recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `MEDIUM_CHUNK_SIZE` para definir el tamaño de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaño sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplea el componente **EmojiThreadAnalyzer** que se encarga de leer y procesar el archivo JSON en fragmentos de tamaño definido por `MEDIUM_CHUNK_SIZE` en fomra concurrente utilizando la librería `concurrent.futures`.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el emoji y su respectivo conteo. Además, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **Obtención de Argumentos y Configuración Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicación mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolección de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecución del código.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la función `q2_time(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario más activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **Finalización del Perfilado y Visualización de Estadísticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadísticas obtenidas durante la ejecución.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepción, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo información completa de la excepción.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la función q2_time(file_path)\n",
    "1. **Determinación del número de workers:**\n",
    "   - `num_workers = multiprocessing.cpu_count()`\n",
    "   - Se obtiene el número de núcleos de CPU disponibles en el sistema, lo que permite definir cuántos hilos se utilizarán para el procesamiento en paralelo.\n",
    "\n",
    "2. **Instanciación del Analizador:**\n",
    "   - `analyzer = EmojiThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)`\n",
    "   - Se crea una instancia de la clase `EmojiThreadAnalyzer`, que se encarga de leer y procesar el archivo JSON en fragmentos de tamaño definido por `MEDIUM_CHUNK_SIZE`.\n",
    "   - El parámetro `file_path` indica la ruta del archivo a procesar.\n",
    "   - `num_workers` se utiliza para configurar el número de hilos que se ejecutarán en paralelo, optimizando así el procesamiento.\n",
    "\n",
    "3. **Análisis y Agregación de Datos:**\n",
    "   - `results = analyzer.analyze()`\n",
    "   - Se invoca el método `analyze()` del objeto `analyzer`, el cual procesa los datos en paralelo y agrega los resultados. Estos resultados incluyen la identificación de los 10 emojis más utilizados con su respectivo conteo.\n",
    "\n",
    "4. **Visualización y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la función retorna los resultados obtenidos.\n",
    "\n",
    "## Resumen\n",
    "Este código sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeños fragmentos.\n",
    "- **Acelerar el procesamiento:** utiliza concurrencia para acelerar el procesamiento de grandes volúmenes de datos en un archivo JSON.\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla ccontiene el emoji y su respectivo conteo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 7286),\n",
      " ('😂', 3072),\n",
      " ('🚜', 2972),\n",
      " ('✊', 2411),\n",
      " ('🌾', 2363),\n",
      " ('🏻', 2080),\n",
      " ('❤', 1779),\n",
      " ('🤣', 1668),\n",
      " ('🏽', 1218),\n",
      " ('👇', 1108)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    228.8 MiB    228.8 MiB           1   @profile\n",
      "    16                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q2_time(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file concurrently to extract the top 10 most used emojis (Focus on optimizing time).\n",
      "    20                                             RESULT: List[Tuple[str, int]]\n",
      "    21                                             \"\"\"\n",
      "    22    228.8 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    23    228.8 MiB      0.0 MiB           1       analyzer = EmojiThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)\n",
      "    24    189.8 MiB    -39.0 MiB           1       results = analyzer.analyze()\n",
      "    25                                         \n",
      "    26    189.5 MiB     -0.2 MiB           1       pprint(results, sort_dicts=False)\n",
      "    27    189.5 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 10.635716 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q2_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción General: q3_memory(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q3_memory.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON con el fin de extraer los 10 usuarios más mencionados, optimizando el uso de memoria.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La función recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `SMALL_CHUNK_SIZE` para definir el tamaño de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaño sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplean tres componentes principales:\n",
    "  - **JsonChunkReader:** Se encarga de leer el archivo JSON por partes, facilitando el manejo de grandes volúmenes de datos.\n",
    "  - **UserAggregator:** Realiza la agregación de datos, como el conteo usuarios de los usuarios más mencionados.\n",
    "  - **UserAnalyzer:** Utiliza el lector y el agregador para analizar la información y determinar los 10 usuarios más mencionados.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el nombre del usuario y el conteo de las menciones. Además, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **Obtención de Argumentos y Configuración Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicación mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolección de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecución del código.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la función `q3_memory(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario más activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **Finalización del Perfilado y Visualización de Estadísticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadísticas obtenidas durante la ejecución.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepción, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo información completa de la excepción.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la función q3_memory(file_path)\n",
    "1. **Inicialización de Componentes:**\n",
    "   - **`JsonChunkReader(file_path, SMALL_CHUNK_SIZE)`**  \n",
    "     Se crea una instancia de `JsonChunkReader` que se encarga de leer el archivo JSON en fragmentos (chunks) pequeños, definidos por `SMALL_CHUNK_SIZE`. Esto permite manejar archivos de gran tamaño sin sobrecargar la memoria.\n",
    "     \n",
    "   - **`UserAggregator()`**  \n",
    "     Se instancia un objeto encargado de agrupar y contar las menciones de cada usuario a medida que se procesan los fragmentos. El agregador recopila los datos relevantes para luego identificar los usuarios más mencionados.\n",
    "     \n",
    "   - **`UserAnalyzer(reader, aggregator)`**  \n",
    "     Se crea el analizador que utiliza el lector y el agregador para procesar el archivo. El analizador coordina la lectura y el conteo de usuarios para finalmente identificar los 10 usuarios con más menciones.\n",
    "  \n",
    "2. **Análisis y Agregación de Datos:**\n",
    "   - **`results = analyzer.analyze()`**  \n",
    "     Se llama al método `analyze()` del analizador. Este método procesa cada fragmento del archivo, utiliza el agregador para contar las ocurrencias de los usuarios y finalmente devuelve una lista de tuplas. Cada tupla contiene un usuario (tipo `str`) y el número de veces que aparece (tipo `int`) utilizando métodos de la clase UserAggregator.\n",
    "\n",
    "3. **Visualización y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la función retorna los resultados obtenidos, una lista de tuplas `(usuario, conteo)`.\n",
    "\n",
    "## Resumen\n",
    "Este código sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeños fragmentos.\n",
    "- **Facilitar el mantenimiento:** Separando la lógica en componentes específicos (lector, agregador y analizador).\n",
    "- **Obtener resultados claros:** Retornando una lista los 10 usuarios más mencionados y sus respectivos conteos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261),\n",
      " ('Kisanektamorcha', 1836),\n",
      " ('RakeshTikaitBKU', 1639),\n",
      " ('PMOIndia', 1422),\n",
      " ('RahulGandhi', 1125),\n",
      " ('GretaThunberg', 1046),\n",
      " ('RaviSinghKA', 1015),\n",
      " ('rihanna', 972),\n",
      " ('UNHumanRights', 962),\n",
      " ('meenaharris', 925)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16    151.6 MiB    151.6 MiB           1   @profile\n",
      "    17                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    18                                             \"\"\"\n",
      "    19                                             CALL: q3_memory(file_path: str)\n",
      "    20                                             DESCRIPTION: Processes a JSON file to extract the top 10 mentioned users (Focus on optimizing memory).\n",
      "    21                                             RESULT: List[Tuple[str, int]\n",
      "    22                                             \"\"\"\n",
      "    23    151.6 MiB      0.0 MiB           1       reader = JsonChunkReader(file_path, SMALL_CHUNK_SIZE)\n",
      "    24    151.6 MiB      0.0 MiB           1       aggregator = UserAggregator()\n",
      "    25    151.6 MiB      0.0 MiB           1       analyzer = UserAnalyzer(reader, aggregator)\n",
      "    26    391.3 MiB    239.7 MiB           1       results = analyzer.analyze()\n",
      "    27                                             \n",
      "    28    391.3 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    29    391.3 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 11.655779 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q3_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción General: q3_time(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q3_time.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON con el fin de extraer los 10 usuarios más mencionados, optimizando el tiempo.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La función recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `MEDIUM_CHUNK_SIZE` para definir el tamaño de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaño sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplea el componente **UserThreadAnalyzer** que se encarga de leer y procesar el archivo JSON en fragmentos de tamaño definido por `MEDIUM_CHUNK_SIZE` en fomra concurrente utilizando la librería `concurrent.futures`.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el nombre del usuario y el conteo de las menciones. Además, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **Obtención de Argumentos y Configuración Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicación mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolección de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecución del código.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la función `q3_time(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario más activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **Finalización del Perfilado y Visualización de Estadísticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadísticas obtenidas durante la ejecución.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepción, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo información completa de la excepción.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la función q3_time(file_path)\n",
    "1. **Determinación del número de workers:**\n",
    "   - `num_workers = multiprocessing.cpu_count()`\n",
    "   - Se obtiene el número de núcleos de CPU disponibles en el sistema, lo que permite definir cuántos hilos se utilizarán para el procesamiento en paralelo.\n",
    "\n",
    "2. **Instanciación del Analizador:**\n",
    "   - `analyzer = UserThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)`\n",
    "   - Se crea una instancia de la clase `UserThreadAnalyzer`, que se encarga de leer y procesar el archivo JSON en fragmentos de tamaño definido por `MEDIUM_CHUNK_SIZE`.\n",
    "   - El parámetro `file_path` indica la ruta del archivo a procesar.\n",
    "   - `num_workers` se utiliza para configurar el número de hilos que se ejecutarán en paralelo, optimizando así el procesamiento.\n",
    "\n",
    "3. **Análisis y Agregación de Datos:**\n",
    "   - `results = analyzer.analyze()`\n",
    "   - Se invoca el método `analyze()` del objeto `analyzer`, el cual procesa los datos en paralelo y agrega los resultados. Estos resultados incluyen la identificación de los 10 usuarios más mencionados con su respectivo conteo.\n",
    "\n",
    "4. **Visualización y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la función retorna los resultados obtenidos.\n",
    "\n",
    "## Resumen\n",
    "Este código sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeños fragmentos.\n",
    "- **Acelerar el procesamiento:** utiliza concurrencia para acelerar el procesamiento de grandes volúmenes de datos en un archivo JSON.\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla contiene el nombre del usuario y el conteo de las menciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261),\n",
      " ('Kisanektamorcha', 1836),\n",
      " ('RakeshTikaitBKU', 1639),\n",
      " ('PMOIndia', 1422),\n",
      " ('RahulGandhi', 1125),\n",
      " ('GretaThunberg', 1046),\n",
      " ('RaviSinghKA', 1015),\n",
      " ('rihanna', 972),\n",
      " ('UNHumanRights', 962),\n",
      " ('meenaharris', 925)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    267.6 MiB    267.6 MiB           1   @profile\n",
      "    16                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q3_time(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file to extract the top 10 mentioned users (Focus on optimizing time).\n",
      "    20                                             RESULT: List[Tuple[str, int]\n",
      "    21                                             \"\"\"\n",
      "    22    267.6 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    23    267.6 MiB      0.0 MiB           1       analyzer = UserThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)\n",
      "    24    313.9 MiB     46.4 MiB           1       results = analyzer.analyze()\n",
      "    25                                             \n",
      "    26    313.7 MiB     -0.2 MiB           1       pprint(results, sort_dicts=False)\n",
      "    27    313.7 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 9.728482 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q3_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
