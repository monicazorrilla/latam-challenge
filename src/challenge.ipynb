{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q1 memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
      " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
      " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
      " (datetime.date(2021, 2, 16), 'jot__b'),\n",
      " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
      " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
      " (datetime.date(2021, 2, 15), 'jot__b'),\n",
      " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
      " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
      " (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15     99.3 MiB     99.3 MiB           1   @profile\n",
      "    16                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    17     99.3 MiB      0.0 MiB           1       chunk_size = SMALL_CHUNK_SIZE\n",
      "    18     99.4 MiB      0.1 MiB           1       chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
      "    19     99.4 MiB      0.0 MiB           1       date_counts = Counter() \n",
      "    20     99.4 MiB      0.0 MiB           1       date_user_counts = defaultdict(Counter)\n",
      "    21                                             \n",
      "    22    249.9 MiB    513.0 MiB          13       for chunk in chunks:\n",
      "    23    250.4 MiB    -23.9 MiB          12           chunk['date'] = chunk['date'].dt.date\n",
      "    24                                                 \n",
      "    25    250.4 MiB    -27.6 MiB          12           date_counts.update(chunk['date'].value_counts().to_dict())\n",
      "    26                                                 \n",
      "    27    250.7 MiB -729504.1 MiB      352233           for date, user in zip(chunk['date'], chunk['user'].apply(lambda u: u.get('username'))):\n",
      "    28    250.7 MiB -239763.6 MiB      117407               if user: \n",
      "    29    250.7 MiB -239761.0 MiB      117407                   date_user_counts[date][user] += 1\n",
      "    30                                                     \n",
      "    31    211.0 MiB   -436.0 MiB          12           del chunk\n",
      "    32    211.0 MiB     -7.3 MiB          12           gc.collect()\n",
      "    33                                                 \n",
      "    34    209.6 MiB    -40.3 MiB           1       top_10_dates = date_counts.most_common(10)\n",
      "    35    209.6 MiB      0.0 MiB           1       results: List = []\n",
      "    36                                             \n",
      "    37    209.6 MiB      0.0 MiB          11       for date, _ in top_10_dates:\n",
      "    38    209.6 MiB      0.0 MiB          10           top_user = date_user_counts[date].most_common(1)\n",
      "    39    209.6 MiB      0.0 MiB          10           if top_user:\n",
      "    40    209.6 MiB      0.0 MiB          10               user_name, _ = top_user[0]\n",
      "    41    209.6 MiB      0.0 MiB          10               results.append((date, user_name))\n",
      "    42                                         \n",
      "    43    209.6 MiB      0.0 MiB           1       del chunks\n",
      "    44    209.6 MiB      0.0 MiB           1       del date_counts\n",
      "    45    130.4 MiB    -79.2 MiB           1       del date_user_counts\n",
      "    46    130.4 MiB      0.0 MiB           1       gc.collect()\n",
      "    47                                             \n",
      "    48    130.4 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    49    130.4 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 22.779326 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q1_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q1 time!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
      " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
      " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
      " (datetime.date(2021, 2, 16), 'jot__b'),\n",
      " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
      " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
      " (datetime.date(2021, 2, 15), 'jot__b'),\n",
      " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
      " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
      " (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    31    122.1 MiB    122.1 MiB           1   @profile    \n",
      "    32                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    33    122.1 MiB      0.0 MiB           1       chunk_size = MEDIUM_CHUNK_SIZE\n",
      "    34    122.1 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    35    122.1 MiB      0.0 MiB           1       date_counts = Counter()  \n",
      "    36    122.1 MiB      0.0 MiB           1       date_user_counts = defaultdict(Counter)  \n",
      "    37                                                 \n",
      "    38    122.1 MiB      0.0 MiB           1       with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor: \n",
      "    39    122.1 MiB      0.0 MiB           1           futures = []\n",
      "    40    122.1 MiB      0.0 MiB           1           chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
      "    41                                                 \n",
      "    42    594.8 MiB    432.8 MiB           7           for chunk in chunks:\n",
      "    43    594.8 MiB    -26.0 MiB           6               futures.append(executor.submit(process_chunk, chunk))\n",
      "    44                                             \n",
      "    45    584.4 MiB    -12.6 MiB           7           for future in concurrent.futures.as_completed(futures):\n",
      "    46    584.1 MiB      0.0 MiB           6               local_date_counts, local_date_user_counts = future.result()\n",
      "    47    584.1 MiB      0.0 MiB           6               date_counts.update(local_date_counts)\n",
      "    48    584.4 MiB      0.0 MiB          24               for date, user_counts in local_date_user_counts.items():\n",
      "    49    584.4 MiB      2.1 MiB          18                   date_user_counts[date].update(user_counts)\n",
      "    50                                             \n",
      "    51    584.4 MiB      0.0 MiB           1       top_10_dates = date_counts.most_common(10)\n",
      "    52    584.4 MiB      0.0 MiB           1       results: List = []\n",
      "    53                                             \n",
      "    54    584.4 MiB      0.0 MiB          11       for date, _ in top_10_dates:\n",
      "    55    584.4 MiB      0.0 MiB          10           top_user = date_user_counts[date].most_common(1)\n",
      "    56    584.4 MiB      0.0 MiB          10           if top_user:\n",
      "    57    584.4 MiB      0.0 MiB          10               user_name, _ = top_user[0]\n",
      "    58    584.4 MiB      0.0 MiB          10               results.append((date, user_name))\n",
      "    59                                         \n",
      "    60    584.4 MiB      0.0 MiB           1       del chunks\n",
      "    61    584.4 MiB      0.0 MiB           1       del date_counts\n",
      "    62    584.4 MiB      0.0 MiB           1       del local_date_counts\n",
      "    63    584.4 MiB      0.0 MiB           1       del date_user_counts\n",
      "    64    584.4 MiB      0.0 MiB           1       del local_date_user_counts\n",
      "    65    584.4 MiB      0.0 MiB           1       gc.collect()  \n",
      "    66                                             \n",
      "    67    584.4 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    68    584.4 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 9.636027 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q1_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q2 memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ðŸ™', 7286),\n",
      " ('ðŸ˜‚', 3072),\n",
      " ('ðŸšœ', 2972),\n",
      " ('âœŠ', 2411),\n",
      " ('ðŸŒ¾', 2363),\n",
      " ('ðŸ»', 2080),\n",
      " ('â¤', 1779),\n",
      " ('ðŸ¤£', 1668),\n",
      " ('ðŸ½', 1218),\n",
      " ('ðŸ‘‡', 1108)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    139.2 MiB    139.2 MiB           1   @profile\n",
      "    16                                         def q2_memory(file_path: str)-> List[Tuple[str, int]]:\n",
      "    17    139.2 MiB      0.0 MiB           1       chunk_size = SMALL_CHUNK_SIZE\n",
      "    18    139.2 MiB      0.0 MiB           1       chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
      "    19                                             \n",
      "    20                                             # Initialize global counters\n",
      "    21    139.2 MiB      0.0 MiB           1       emoji_counter = Counter()\n",
      "    22                                             \n",
      "    23    284.2 MiB    302.6 MiB          13       for chunk in chunks:\n",
      "    24                                                 # Extract emojis from content\n",
      "    25    284.2 MiB    -23.9 MiB          12           chunk['emojis'] = chunk['content'].astype(str).map(extract_emojis)\n",
      "    26                                         \n",
      "    27                                                 # Update the counter with emojis from each chunk\n",
      "    28    284.2 MiB -231585.0 MiB      117419           for emojis in chunk['emojis']:\n",
      "    29    284.2 MiB -231560.6 MiB      117407               emoji_counter.update(emojis)\n",
      "    30                                                 \n",
      "    31                                                 # Force memory cleanup after each chunk\n",
      "    32    280.0 MiB   -182.7 MiB          12           del chunk\n",
      "    33    279.4 MiB     -5.5 MiB          12           gc.collect()  # Force garbage collection\n",
      "    34                                         \n",
      "    35                                             # Get the top 10 most used emojis\n",
      "    36    279.3 MiB     -4.9 MiB           1       top_10_emojis = emoji_counter.most_common(10)\n",
      "    37    279.3 MiB      0.0 MiB           1       results: List = []\n",
      "    38                                         \n",
      "    39    279.3 MiB      0.0 MiB          11       for emoji_char, count in top_10_emojis:\n",
      "    40    279.3 MiB      0.0 MiB          10           results.append((emoji_char, count))\n",
      "    41                                         \n",
      "    42                                             # Clear large objects to prevent memory buildup\n",
      "    43    279.3 MiB      0.0 MiB           1       del chunks\n",
      "    44    224.3 MiB    -55.0 MiB           1       del emoji_counter\n",
      "    45    224.3 MiB      0.0 MiB           1       gc.collect() \n",
      "    46                                             \n",
      "    47    224.4 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    48    224.4 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 22.818913 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q2_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q2 time!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ðŸ™', 7286),\n",
      " ('ðŸ˜‚', 3072),\n",
      " ('ðŸšœ', 2972),\n",
      " ('âœŠ', 2411),\n",
      " ('ðŸŒ¾', 2363),\n",
      " ('ðŸ»', 2080),\n",
      " ('â¤', 1779),\n",
      " ('ðŸ¤£', 1668),\n",
      " ('ðŸ½', 1218),\n",
      " ('ðŸ‘‡', 1108)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    30    222.6 MiB    222.6 MiB           1   @profile\n",
      "    31                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    32    222.6 MiB      0.0 MiB           1       chunk_size = MEDIUM_CHUNK_SIZE\n",
      "    33    222.6 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    34                                             \n",
      "    35                                             # Initialize global counters\n",
      "    36    222.6 MiB      0.0 MiB           1       emoji_counter = Counter()\n",
      "    37                                             \n",
      "    38    222.6 MiB      0.0 MiB           1       with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
      "    39    222.6 MiB      0.0 MiB           1           futures = []\n",
      "    40    222.6 MiB      0.0 MiB           1           chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
      "    41                                                 \n",
      "    42    678.3 MiB    420.6 MiB           7           for chunk in chunks:\n",
      "    43    678.3 MiB    -17.5 MiB           6               futures.append(executor.submit(process_chunk, chunk))\n",
      "    44                                             \n",
      "    45    660.8 MiB    -17.5 MiB           7           for future in concurrent.futures.as_completed(futures):\n",
      "    46    660.8 MiB      0.0 MiB           6               emoji_counter.update(future.result())\n",
      "    47                                         \n",
      "    48                                             # Get the top 10 most used emojis\n",
      "    49    660.7 MiB     -0.0 MiB           1       top_10_emojis = emoji_counter.most_common(10)\n",
      "    50    660.7 MiB      0.0 MiB           1       results: List = []\n",
      "    51                                         \n",
      "    52    660.7 MiB      0.0 MiB          11       for emoji_char, count in top_10_emojis:\n",
      "    53    660.7 MiB      0.0 MiB          10           results.append((emoji_char, count))\n",
      "    54                                         \n",
      "    55                                             # Clear large objects to prevent memory buildup\n",
      "    56    660.7 MiB      0.0 MiB           1       del chunks\n",
      "    57    660.7 MiB      0.0 MiB           1       del emoji_counter\n",
      "    58    660.7 MiB      0.0 MiB           1       gc.collect()  # Force garbage collection\n",
      "    59                                             \n",
      "    60    660.7 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    61    660.7 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 10.736764 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q2_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q3 memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261),\n",
      " ('Kisanektamorcha', 1836),\n",
      " ('RakeshTikaitBKU', 1639),\n",
      " ('PMOIndia', 1422),\n",
      " ('RahulGandhi', 1125),\n",
      " ('GretaThunberg', 1046),\n",
      " ('RaviSinghKA', 1015),\n",
      " ('rihanna', 972),\n",
      " ('UNHumanRights', 962),\n",
      " ('meenaharris', 925)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    152.5 MiB    152.5 MiB           1   @profile\n",
      "    16                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    17    152.5 MiB      0.0 MiB           1       chunk_size = SMALL_CHUNK_SIZE\n",
      "    18    152.5 MiB      0.0 MiB           1       chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
      "    19                                         \n",
      "    20                                             # Initialize mention counter\n",
      "    21    152.5 MiB      0.0 MiB           1       mention_counter = Counter()\n",
      "    22                                         \n",
      "    23    281.8 MiB    641.5 MiB          13       for chunk in chunks:\n",
      "    24    281.9 MiB    -45.5 MiB          12           chunk['mentions'] = chunk['content'].dropna().map(extract_mentions)\n",
      "    25                                                 # Update the mention counter\n",
      "    26    281.9 MiB -399981.0 MiB      117419           for mentions in chunk['mentions']:\n",
      "    27    281.9 MiB -399933.9 MiB      117407               mention_counter.update(mentions)\n",
      "    28                                         \n",
      "    29                                                 # Force memory cleanup after each chunk\n",
      "    30    236.8 MiB   -602.4 MiB          12           del chunk\n",
      "    31    236.6 MiB     -3.5 MiB          12           gc.collect()  # Force garbage collection\n",
      "    32                                             \n",
      "    33                                             # Get the Top 10 most mentioned username\n",
      "    34    236.6 MiB    -45.3 MiB           1       top_10_users = mention_counter.most_common(10)\n",
      "    35    236.6 MiB      0.0 MiB           1       results: List = []\n",
      "    36                                         \n",
      "    37    236.6 MiB      0.0 MiB          11       for username, count in top_10_users:\n",
      "    38    236.6 MiB      0.0 MiB          10           results.append((username, count))\n",
      "    39                                         \n",
      "    40                                             # Clear large objects to prevent memory buildup\n",
      "    41    236.6 MiB      0.0 MiB           1       del chunks\n",
      "    42    233.3 MiB     -3.2 MiB           1       del mention_counter\n",
      "    43    233.3 MiB      0.0 MiB           1       gc.collect()  \n",
      "    44                                             \n",
      "    45    233.3 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    46    233.3 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 15.435184 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q3_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q3 time!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261),\n",
      " ('Kisanektamorcha', 1836),\n",
      " ('RakeshTikaitBKU', 1639),\n",
      " ('PMOIndia', 1422),\n",
      " ('RahulGandhi', 1125),\n",
      " ('GretaThunberg', 1046),\n",
      " ('RaviSinghKA', 1015),\n",
      " ('rihanna', 972),\n",
      " ('UNHumanRights', 962),\n",
      " ('meenaharris', 925)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    29    231.4 MiB    231.4 MiB           1   @profile\n",
      "    30                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    31    231.4 MiB      0.0 MiB           1       chunk_size = MEDIUM_CHUNK_SIZE\n",
      "    32    231.4 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    33                                             \n",
      "    34                                             # Initialize global counters\n",
      "    35    231.4 MiB      0.0 MiB           1       mention_counter = Counter()\n",
      "    36                                         \n",
      "    37    231.4 MiB      0.0 MiB           1       with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
      "    38    231.4 MiB      0.0 MiB           1           futures = []\n",
      "    39    231.4 MiB      0.0 MiB           1           chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
      "    40                                                 \n",
      "    41                                                 # Read JSON file in chunks and process in parallel\n",
      "    42    680.7 MiB    392.3 MiB           7           for chunk in chunks:\n",
      "    43    680.7 MiB    -32.6 MiB           6               futures.append(executor.submit(process_chunk, chunk))\n",
      "    44                                         \n",
      "    45                                                 # Aggregate results from all threads\n",
      "    46    656.4 MiB    -24.3 MiB           7           for future in concurrent.futures.as_completed(futures):\n",
      "    47    656.4 MiB      0.0 MiB           6               mention_counter.update(future.result())\n",
      "    48                                         \n",
      "    49                                             # Get the Top 10 most mentioned username\n",
      "    50    656.4 MiB     -0.0 MiB           1       top_10_users = mention_counter.most_common(10)\n",
      "    51    656.4 MiB      0.0 MiB           1       results: List = []\n",
      "    52                                         \n",
      "    53    656.4 MiB      0.0 MiB          11       for username, count in top_10_users:\n",
      "    54    656.4 MiB      0.0 MiB          10           results.append((username, count))\n",
      "    55                                         \n",
      "    56                                             # Clear large objects to prevent memory buildup\n",
      "    57    656.4 MiB      0.0 MiB           1       del chunks\n",
      "    58    656.4 MiB      0.0 MiB           1       del mention_counter\n",
      "    59    656.4 MiB      0.0 MiB           1       gc.collect()  \n",
      "    60                                             \n",
      "    61    656.4 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    62    656.4 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 9.495008 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q3_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
