{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DescripciÃ³n General: q1_memory(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q1_memory.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para analizar la actividad diaria y determinar el usuario mÃ¡s activo por fecha, optimizando el uso de memoria.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La funciÃ³n recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `SMALL_CHUNK_SIZE` para definir el tamaÃ±o de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaÃ±o sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplean tres componentes principales:\n",
    "  - **JsonChunkReader:** Se encarga de leer el archivo JSON por partes, facilitando el manejo de grandes volÃºmenes de datos.\n",
    "  - **TweetAggregator:** Realiza la agregaciÃ³n de datos, como el conteo de tweets por fecha y por usuario.\n",
    "  - **TweetAnalyzer:** Utiliza el lector y el agregador para analizar la informaciÃ³n y determinar, entre otros resultados, cuÃ¡l es el usuario mÃ¡s activo en las fechas de mayor actividad.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene la fecha y el nombre del usuario mÃ¡s activo en esa fecha. AdemÃ¡s, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **ObtenciÃ³n de Argumentos y ConfiguraciÃ³n Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicaciÃ³n mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolecciÃ³n de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecuciÃ³n del cÃ³digo.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la funciÃ³n `q1_memory(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario mÃ¡s activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **FinalizaciÃ³n del Perfilado y VisualizaciÃ³n de EstadÃ­sticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadÃ­sticas obtenidas durante la ejecuciÃ³n.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepciÃ³n, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo informaciÃ³n completa de la excepciÃ³n.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la funciÃ³n q1_memory(file_path)\n",
    "1. **InicializaciÃ³n de Componentes:**\n",
    "   - Se crea una instancia de `JsonChunkReader` pasando la ruta del archivo (file_path) y el tamaÃ±o de fragmento (`SMALL_CHUNK_SIZE`).\n",
    "   - Se instancia un objeto `TweetAggregator` que se encargarÃ¡ de agrupar y contar los datos.\n",
    "   - Se crea un objeto `TweetAnalyzer` al que se le suministran el lector y el agregador, estableciendo asÃ­ la conexiÃ³n entre la lectura y el anÃ¡lisis de los datos.\n",
    "\n",
    "2. **AnÃ¡lisis y AgregaciÃ³n de Datos:**\n",
    "   - Se llama al mÃ©todo `analyze()` del objeto `TweetAnalyzer`. Este mÃ©todo procesa los datos del archivo en fragmentos, contando los tweets por fecha y determinando el usuario mÃ¡s activo de cada una de las 10 fechas con mayor actividad utilizando mÃ©todos de la clase `TweetAggregator`.\n",
    "\n",
    "3. **VisualizaciÃ³n y Retorno de Resultados:**\n",
    "   - Los resultados, que consisten en una lista de tuplas `(fecha, usuario)`, se imprimen utilizando `pprint` para facilitar su visualizaciÃ³n.\n",
    "   - Finalmente, la funciÃ³n retorna estos resultados.\n",
    "\n",
    "## Resumen\n",
    "Este cÃ³digo sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeÃ±os fragmentos.\n",
    "- **Facilitar el mantenimiento:** Separando la lÃ³gica en componentes especÃ­ficos (lector, agregador y analizador).\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla contiene la fecha y el nombre del usuario mÃ¡s activo en esa fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
      " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
      " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
      " (datetime.date(2021, 2, 16), 'jot__b'),\n",
      " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
      " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
      " (datetime.date(2021, 2, 15), 'jot__b'),\n",
      " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
      " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
      " (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    17     96.1 MiB     96.1 MiB           1   @profile\n",
      "    18                                         def q1_memory(file_path: str) -> List[Tuple[date, str]]:\n",
      "    19                                             \"\"\"\n",
      "    20                                             CALL: q1_memory(file_path: str)\n",
      "    21                                             DESCRIPTION: Processes a JSON file to extract the top user for each of the top 10 dates (Focus on optimizing memory).\n",
      "    22                                             RESULT: List[Tuple[date, str]]\n",
      "    23                                             \"\"\"\n",
      "    24     96.1 MiB      0.0 MiB           1       reader = JsonChunkReader(file_path, SMALL_CHUNK_SIZE)\n",
      "    25     96.1 MiB      0.0 MiB           1       aggregator = TweetAggregator()\n",
      "    26     96.1 MiB      0.0 MiB           1       analyzer = TweetAnalyzer(reader, aggregator)\n",
      "    27    205.6 MiB    109.4 MiB           1       results = analyzer.analyze()\n",
      "    28                                         \n",
      "    29    205.6 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    30    205.6 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 10.249084 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q1_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DescripciÃ³n General: q1_time(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q1_time.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para analizar la actividad diaria y determinar el usuario mÃ¡s activo por fecha, optimizando el tiempo.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La funciÃ³n recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `MEDIUM_CHUNK_SIZE` para definir el tamaÃ±o de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaÃ±o sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplea el componente **TweetThreadAnalyzer** que se encarga de leer y procesar el archivo JSON en fragmentos de tamaÃ±o definido por `MEDIUM_CHUNK_SIZE` en fomra concurrente utilizando la librerÃ­a `concurrent.futures`.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene la fecha y el nombre del usuario mÃ¡s activo en esa fecha. AdemÃ¡s, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **ObtenciÃ³n de Argumentos y ConfiguraciÃ³n Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicaciÃ³n mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolecciÃ³n de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecuciÃ³n del cÃ³digo.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la funciÃ³n `q1_time(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario mÃ¡s activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **FinalizaciÃ³n del Perfilado y VisualizaciÃ³n de EstadÃ­sticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadÃ­sticas obtenidas durante la ejecuciÃ³n.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepciÃ³n, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo informaciÃ³n completa de la excepciÃ³n.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la funciÃ³n q1_time(file_path)\n",
    "1. **DeterminaciÃ³n del nÃºmero de workers:**\n",
    "   - `num_workers = multiprocessing.cpu_count()`\n",
    "   - Se obtiene el nÃºmero de nÃºcleos de CPU disponibles en el sistema, lo que permite definir cuÃ¡ntos hilos se utilizarÃ¡n para el procesamiento en paralelo.\n",
    "\n",
    "2. **InstanciaciÃ³n del Analizador:**\n",
    "   - `analyzer = TweetThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)`\n",
    "   - Se crea una instancia de la clase `TweetThreadAnalyzer`, que se encarga de leer y procesar el archivo JSON en fragmentos de tamaÃ±o definido por `MEDIUM_CHUNK_SIZE`.\n",
    "   - El parÃ¡metro `file_path` indica la ruta del archivo a procesar.\n",
    "   - `num_workers` se utiliza para configurar el nÃºmero de hilos que se ejecutarÃ¡n en paralelo, optimizando asÃ­ el procesamiento.\n",
    "\n",
    "3. **AnÃ¡lisis y AgregaciÃ³n de Datos:**\n",
    "   - `results = analyzer.analyze()`\n",
    "   - Se invoca el mÃ©todo `analyze()` del objeto `analyzer`, el cual procesa los datos en paralelo y agrega los resultados. Estos resultados incluyen la identificaciÃ³n de las fechas con mayor actividad y el usuario mÃ¡s activo en cada una de ellas.\n",
    "\n",
    "4. **VisualizaciÃ³n y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la funciÃ³n retorna los resultados obtenidos.\n",
    "\n",
    "## Resumen\n",
    "Este cÃ³digo sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeÃ±os fragmentos.\n",
    "- **Acelerar el procesamiento:** utiliza concurrencia para acelerar el procesamiento de grandes volÃºmenes de datos en un archivo JSON.\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla contiene la fecha y el nombre del usuario mÃ¡s activo en esa fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
      " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
      " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
      " (datetime.date(2021, 2, 16), 'jot__b'),\n",
      " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
      " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
      " (datetime.date(2021, 2, 15), 'jot__b'),\n",
      " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
      " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
      " (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    117.6 MiB    117.6 MiB           1   @profile    \n",
      "    16                                         def q1_time(file_path: str) -> List[Tuple[date, str]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q1_time(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file concurrently to extract the top user for each of the top 10 dates (Focus on optimizing time).\n",
      "    20                                             RESULT: List[Tuple[date, str]]\n",
      "    21                                             \"\"\"\n",
      "    22    117.6 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    23    117.6 MiB      0.0 MiB           1       analyzer = TweetThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)\n",
      "    24    169.9 MiB     52.4 MiB           1       results = analyzer.analyze()\n",
      "    25                                         \n",
      "    26    169.7 MiB     -0.2 MiB           1       pprint(results, sort_dicts=False)\n",
      "    27    169.7 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 9.930427 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q1_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DescripciÃ³n General: q2_memory(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q2_memory.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON y extraer los 10 emojis mÃ¡s utilizados, optimizando el uso de memoria.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La funciÃ³n recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `SMALL_CHUNK_SIZE` para definir el tamaÃ±o de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaÃ±o sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplean tres componentes principales:\n",
    "  - **JsonChunkReader:** Se encarga de leer el archivo JSON por partes, facilitando el manejo de grandes volÃºmenes de datos.\n",
    "  - **EmojiAggregator:** Realiza la agregaciÃ³n de datos, como el conteo emojis mÃ¡s utilizados.\n",
    "  - **EmojiAnalyzer:** Utiliza el lector y el agregador para analizar la informaciÃ³n y determinar los 10 emojis mÃ¡s utilizados con su respectivo conteo.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el emoji y su respectivo conteo. AdemÃ¡s, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **ObtenciÃ³n de Argumentos y ConfiguraciÃ³n Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicaciÃ³n mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolecciÃ³n de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecuciÃ³n del cÃ³digo.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la funciÃ³n `q2_memory(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario mÃ¡s activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **FinalizaciÃ³n del Perfilado y VisualizaciÃ³n de EstadÃ­sticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadÃ­sticas obtenidas durante la ejecuciÃ³n.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepciÃ³n, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo informaciÃ³n completa de la excepciÃ³n.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la funciÃ³n q2_memory(file_path)\n",
    "1. **InicializaciÃ³n de Componentes:**\n",
    "   - **`JsonChunkReader(file_path, SMALL_CHUNK_SIZE)`**  \n",
    "     Se crea una instancia de `JsonChunkReader` que se encarga de leer el archivo JSON en fragmentos (chunks) pequeÃ±os, definidos por `SMALL_CHUNK_SIZE`. Esto permite manejar archivos de gran tamaÃ±o sin sobrecargar la memoria. \n",
    "   - **`EmojiAggregator()`**  \n",
    "     Se instancia un objeto encargado de agrupar y contar los emojis extraÃ­dos del archivo. El agregador recopila las ocurrencias de cada emoji a medida que se procesan los fragmentos.\n",
    "   - **`EmojiAnalyzer(reader, aggregator)`**  \n",
    "     Se crea el analizador que utiliza el lector y el agregador para procesar el archivo. El analizador coordina la lectura y el conteo de emojis para finalmente identificar los 10 emojis mÃ¡s utilizados.\n",
    "\n",
    "2. **AnÃ¡lisis y AgregaciÃ³n de Datos:**\n",
    "   - **`results = analyzer.analyze()`**  \n",
    "     Se llama al mÃ©todo `analyze()` del analizador. Este mÃ©todo procesa cada fragmento del archivo, utiliza el agregador para contar las ocurrencias de los emojis y finalmente devuelve una lista de tuplas. Cada tupla contiene un emoji (tipo `str`) y el nÃºmero de veces que aparece (tipo `int`) utilizando mÃ©todos de la clase EmojiAggregator.\n",
    "\n",
    "3. **VisualizaciÃ³n y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la funciÃ³n retorna los resultados obtenidos, una lista de tuplas `(emoji, conteo)`.\n",
    "\n",
    "## Resumen\n",
    "Este cÃ³digo sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeÃ±os fragmentos.\n",
    "- **Facilitar el mantenimiento:** Separando la lÃ³gica en componentes especÃ­ficos (lector, agregador y analizador).\n",
    "- **Obtener resultados claros:** Retornando una lista de los emojis mÃ¡s populares y sus respectivas cantidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ðŸ™', 7286),\n",
      " ('ðŸ˜‚', 3072),\n",
      " ('ðŸšœ', 2972),\n",
      " ('âœŠ', 2411),\n",
      " ('ðŸŒ¾', 2363),\n",
      " ('ðŸ»', 2080),\n",
      " ('â¤', 1779),\n",
      " ('ðŸ¤£', 1668),\n",
      " ('ðŸ½', 1218),\n",
      " ('ðŸ‘‡', 1108)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    132.3 MiB    132.3 MiB           1   @profile\n",
      "    16                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q2_memory(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file to extract the top 10 most used emojis (Focus on optimizing memory).\n",
      "    20                                             RESULT: List[Tuple[str, int]\n",
      "    21                                             \"\"\"\n",
      "    22    132.3 MiB      0.0 MiB           1       reader = JsonChunkReader(file_path, SMALL_CHUNK_SIZE)\n",
      "    23    132.3 MiB      0.0 MiB           1       aggregator = EmojiAggregator()\n",
      "    24    132.3 MiB      0.0 MiB           1       analyzer = EmojiAnalyzer(reader, aggregator)\n",
      "    25    281.7 MiB    149.5 MiB           1       results = analyzer.analyze()\n",
      "    26                                             \n",
      "    27    281.2 MiB     -0.5 MiB           1       pprint(results, sort_dicts=False)\n",
      "    28    281.2 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 19.155698 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q2_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DescripciÃ³n General: q2_time(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q2_time.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON y extraer los 10 emojis mÃ¡s utilizados, optimizando el tiempo.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La funciÃ³n recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `MEDIUM_CHUNK_SIZE` para definir el tamaÃ±o de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaÃ±o sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplea el componente **EmojiThreadAnalyzer** que se encarga de leer y procesar el archivo JSON en fragmentos de tamaÃ±o definido por `MEDIUM_CHUNK_SIZE` en fomra concurrente utilizando la librerÃ­a `concurrent.futures`.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el emoji y su respectivo conteo. AdemÃ¡s, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **ObtenciÃ³n de Argumentos y ConfiguraciÃ³n Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicaciÃ³n mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolecciÃ³n de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecuciÃ³n del cÃ³digo.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la funciÃ³n `q2_time(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario mÃ¡s activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **FinalizaciÃ³n del Perfilado y VisualizaciÃ³n de EstadÃ­sticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadÃ­sticas obtenidas durante la ejecuciÃ³n.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepciÃ³n, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo informaciÃ³n completa de la excepciÃ³n.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la funciÃ³n q2_time(file_path)\n",
    "1. **DeterminaciÃ³n del nÃºmero de workers:**\n",
    "   - `num_workers = multiprocessing.cpu_count()`\n",
    "   - Se obtiene el nÃºmero de nÃºcleos de CPU disponibles en el sistema, lo que permite definir cuÃ¡ntos hilos se utilizarÃ¡n para el procesamiento en paralelo.\n",
    "\n",
    "2. **InstanciaciÃ³n del Analizador:**\n",
    "   - `analyzer = EmojiThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)`\n",
    "   - Se crea una instancia de la clase `EmojiThreadAnalyzer`, que se encarga de leer y procesar el archivo JSON en fragmentos de tamaÃ±o definido por `MEDIUM_CHUNK_SIZE`.\n",
    "   - El parÃ¡metro `file_path` indica la ruta del archivo a procesar.\n",
    "   - `num_workers` se utiliza para configurar el nÃºmero de hilos que se ejecutarÃ¡n en paralelo, optimizando asÃ­ el procesamiento.\n",
    "\n",
    "3. **AnÃ¡lisis y AgregaciÃ³n de Datos:**\n",
    "   - `results = analyzer.analyze()`\n",
    "   - Se invoca el mÃ©todo `analyze()` del objeto `analyzer`, el cual procesa los datos en paralelo y agrega los resultados. Estos resultados incluyen la identificaciÃ³n de los 10 emojis mÃ¡s utilizados con su respectivo conteo.\n",
    "\n",
    "4. **VisualizaciÃ³n y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la funciÃ³n retorna los resultados obtenidos.\n",
    "\n",
    "## Resumen\n",
    "Este cÃ³digo sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeÃ±os fragmentos.\n",
    "- **Acelerar el procesamiento:** utiliza concurrencia para acelerar el procesamiento de grandes volÃºmenes de datos en un archivo JSON.\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla ccontiene el emoji y su respectivo conteo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ðŸ™', 7286),\n",
      " ('ðŸ˜‚', 3072),\n",
      " ('ðŸšœ', 2972),\n",
      " ('âœŠ', 2411),\n",
      " ('ðŸŒ¾', 2363),\n",
      " ('ðŸ»', 2080),\n",
      " ('â¤', 1779),\n",
      " ('ðŸ¤£', 1668),\n",
      " ('ðŸ½', 1218),\n",
      " ('ðŸ‘‡', 1108)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    228.8 MiB    228.8 MiB           1   @profile\n",
      "    16                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q2_time(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file concurrently to extract the top 10 most used emojis (Focus on optimizing time).\n",
      "    20                                             RESULT: List[Tuple[str, int]]\n",
      "    21                                             \"\"\"\n",
      "    22    228.8 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    23    228.8 MiB      0.0 MiB           1       analyzer = EmojiThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)\n",
      "    24    189.8 MiB    -39.0 MiB           1       results = analyzer.analyze()\n",
      "    25                                         \n",
      "    26    189.5 MiB     -0.2 MiB           1       pprint(results, sort_dicts=False)\n",
      "    27    189.5 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 10.635716 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q2_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DescripciÃ³n General: q3_memory(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q3_memory.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON con el fin de extraer los 10 usuarios mÃ¡s mencionados, optimizando el uso de memoria.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La funciÃ³n recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `SMALL_CHUNK_SIZE` para definir el tamaÃ±o de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaÃ±o sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplean tres componentes principales:\n",
    "  - **JsonChunkReader:** Se encarga de leer el archivo JSON por partes, facilitando el manejo de grandes volÃºmenes de datos.\n",
    "  - **UserAggregator:** Realiza la agregaciÃ³n de datos, como el conteo usuarios de los usuarios mÃ¡s mencionados.\n",
    "  - **UserAnalyzer:** Utiliza el lector y el agregador para analizar la informaciÃ³n y determinar los 10 usuarios mÃ¡s mencionados.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el nombre del usuario y el conteo de las menciones. AdemÃ¡s, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **ObtenciÃ³n de Argumentos y ConfiguraciÃ³n Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicaciÃ³n mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolecciÃ³n de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecuciÃ³n del cÃ³digo.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la funciÃ³n `q3_memory(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario mÃ¡s activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **FinalizaciÃ³n del Perfilado y VisualizaciÃ³n de EstadÃ­sticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadÃ­sticas obtenidas durante la ejecuciÃ³n.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepciÃ³n, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo informaciÃ³n completa de la excepciÃ³n.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la funciÃ³n q3_memory(file_path)\n",
    "1. **InicializaciÃ³n de Componentes:**\n",
    "   - **`JsonChunkReader(file_path, SMALL_CHUNK_SIZE)`**  \n",
    "     Se crea una instancia de `JsonChunkReader` que se encarga de leer el archivo JSON en fragmentos (chunks) pequeÃ±os, definidos por `SMALL_CHUNK_SIZE`. Esto permite manejar archivos de gran tamaÃ±o sin sobrecargar la memoria.\n",
    "     \n",
    "   - **`UserAggregator()`**  \n",
    "     Se instancia un objeto encargado de agrupar y contar las menciones de cada usuario a medida que se procesan los fragmentos. El agregador recopila los datos relevantes para luego identificar los usuarios mÃ¡s mencionados.\n",
    "     \n",
    "   - **`UserAnalyzer(reader, aggregator)`**  \n",
    "     Se crea el analizador que utiliza el lector y el agregador para procesar el archivo. El analizador coordina la lectura y el conteo de usuarios para finalmente identificar los 10 usuarios con mÃ¡s menciones.\n",
    "  \n",
    "2. **AnÃ¡lisis y AgregaciÃ³n de Datos:**\n",
    "   - **`results = analyzer.analyze()`**  \n",
    "     Se llama al mÃ©todo `analyze()` del analizador. Este mÃ©todo procesa cada fragmento del archivo, utiliza el agregador para contar las ocurrencias de los usuarios y finalmente devuelve una lista de tuplas. Cada tupla contiene un usuario (tipo `str`) y el nÃºmero de veces que aparece (tipo `int`) utilizando mÃ©todos de la clase UserAggregator.\n",
    "\n",
    "3. **VisualizaciÃ³n y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la funciÃ³n retorna los resultados obtenidos, una lista de tuplas `(usuario, conteo)`.\n",
    "\n",
    "## Resumen\n",
    "Este cÃ³digo sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeÃ±os fragmentos.\n",
    "- **Facilitar el mantenimiento:** Separando la lÃ³gica en componentes especÃ­ficos (lector, agregador y analizador).\n",
    "- **Obtener resultados claros:** Retornando una lista los 10 usuarios mÃ¡s mencionados y sus respectivos conteos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261),\n",
      " ('Kisanektamorcha', 1836),\n",
      " ('RakeshTikaitBKU', 1639),\n",
      " ('PMOIndia', 1422),\n",
      " ('RahulGandhi', 1125),\n",
      " ('GretaThunberg', 1046),\n",
      " ('RaviSinghKA', 1015),\n",
      " ('rihanna', 972),\n",
      " ('UNHumanRights', 962),\n",
      " ('meenaharris', 925)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16    151.6 MiB    151.6 MiB           1   @profile\n",
      "    17                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    18                                             \"\"\"\n",
      "    19                                             CALL: q3_memory(file_path: str)\n",
      "    20                                             DESCRIPTION: Processes a JSON file to extract the top 10 mentioned users (Focus on optimizing memory).\n",
      "    21                                             RESULT: List[Tuple[str, int]\n",
      "    22                                             \"\"\"\n",
      "    23    151.6 MiB      0.0 MiB           1       reader = JsonChunkReader(file_path, SMALL_CHUNK_SIZE)\n",
      "    24    151.6 MiB      0.0 MiB           1       aggregator = UserAggregator()\n",
      "    25    151.6 MiB      0.0 MiB           1       analyzer = UserAnalyzer(reader, aggregator)\n",
      "    26    391.3 MiB    239.7 MiB           1       results = analyzer.analyze()\n",
      "    27                                             \n",
      "    28    391.3 MiB      0.0 MiB           1       pprint(results, sort_dicts=False)\n",
      "    29    391.3 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 11.655779 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q3_memory.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DescripciÃ³n General: q3_time(file_path)\n",
    "- **Objetivo:**\n",
    "  El script q3_time.py procesa un archivo JSON de tweets en \"chunks\" (fragmentos) para procesar un archivo JSON con el fin de extraer los 10 usuarios mÃ¡s mencionados, optimizando el tiempo.\n",
    "  \n",
    "- **Entrada:**  \n",
    "  La funciÃ³n recibe la ruta al archivo JSON (`file_path`) y utiliza la constante `MEDIUM_CHUNK_SIZE` para definir el tamaÃ±o de los fragmentos (chunks) en los que se lee el archivo. Esto permite procesar archivos de gran tamaÃ±o sin sobrecargar la memoria.\n",
    "\n",
    "- **Proceso:**  \n",
    "  Se emplea el componente **UserThreadAnalyzer** que se encarga de leer y procesar el archivo JSON en fragmentos de tamaÃ±o definido por `MEDIUM_CHUNK_SIZE` en fomra concurrente utilizando la librerÃ­a `concurrent.futures`.\n",
    "\n",
    "- **Salida:**  \n",
    "  El resultado es una lista de tuplas, donde cada tupla contiene el nombre del usuario y el conteo de las menciones. AdemÃ¡s, el resultado se imprime en un formato legible mediante `pprint`.\n",
    "\n",
    "### Detalle del Proceso\n",
    "1. **ObtenciÃ³n de Argumentos y ConfiguraciÃ³n Inicial:**\n",
    "   - Se obtienen los argumentos de la aplicaciÃ³n mediante `get_app_args()`, y se extrae la ruta del archivo (`file_path`).\n",
    "   - Se llama a `gc.collect()` para forzar la recolecciÃ³n de basura y liberar memoria antes de iniciar el procesamiento.\n",
    "\n",
    "2. **Inicio del Perfilado del Rendimiento:**\n",
    "   - Se crea una instancia de `cProfile.Profile()` para medir el rendimiento.\n",
    "   - Con `profiler.enable()` se activa el perfilador para comenzar a recopilar datos de ejecuciÃ³n del cÃ³digo.\n",
    "\n",
    "3. **Procesamiento del Archivo:**\n",
    "   - Se llama a la funciÃ³n `q3_time(file_path)`, la cual procesa el archivo JSON para obtener, por ejemplo, el usuario mÃ¡s activo en las 10 fechas con mayor actividad, optimizando el uso de la memoria mediante el procesamiento por fragmentos.\n",
    "\n",
    "4. **FinalizaciÃ³n del Perfilado y VisualizaciÃ³n de EstadÃ­sticas:**\n",
    "   - Una vez finalizado el procesamiento, se desactiva el perfilador con `profiler.disable()`.\n",
    "   - Se llama a `get_stats_in_memory(profiler)` para mostrar o procesar las estadÃ­sticas obtenidas durante la ejecuciÃ³n.\n",
    "\n",
    "5. **Manejo de Errores y Limpieza Final:**\n",
    "   - Si ocurre alguna excepciÃ³n, se captura en el bloque `except` y se registra un mensaje de error detallado mediante `logging.error()`, incluyendo informaciÃ³n completa de la excepciÃ³n.\n",
    "   - Finalmente, en el bloque `finally`, se vuelve a llamar a `gc.collect()` para asegurar que la memoria se libere de manera adecuada.\n",
    "\n",
    "### Detalle en la funciÃ³n q3_time(file_path)\n",
    "1. **DeterminaciÃ³n del nÃºmero de workers:**\n",
    "   - `num_workers = multiprocessing.cpu_count()`\n",
    "   - Se obtiene el nÃºmero de nÃºcleos de CPU disponibles en el sistema, lo que permite definir cuÃ¡ntos hilos se utilizarÃ¡n para el procesamiento en paralelo.\n",
    "\n",
    "2. **InstanciaciÃ³n del Analizador:**\n",
    "   - `analyzer = UserThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)`\n",
    "   - Se crea una instancia de la clase `UserThreadAnalyzer`, que se encarga de leer y procesar el archivo JSON en fragmentos de tamaÃ±o definido por `MEDIUM_CHUNK_SIZE`.\n",
    "   - El parÃ¡metro `file_path` indica la ruta del archivo a procesar.\n",
    "   - `num_workers` se utiliza para configurar el nÃºmero de hilos que se ejecutarÃ¡n en paralelo, optimizando asÃ­ el procesamiento.\n",
    "\n",
    "3. **AnÃ¡lisis y AgregaciÃ³n de Datos:**\n",
    "   - `results = analyzer.analyze()`\n",
    "   - Se invoca el mÃ©todo `analyze()` del objeto `analyzer`, el cual procesa los datos en paralelo y agrega los resultados. Estos resultados incluyen la identificaciÃ³n de los 10 usuarios mÃ¡s mencionados con su respectivo conteo.\n",
    "\n",
    "4. **VisualizaciÃ³n y Retorno de Resultados:**\n",
    "   - `pprint(results, sort_dicts=False)`\n",
    "   - Se utiliza `pprint` para imprimir los resultados de manera clara y ordenada.\n",
    "   - Finalmente, la funciÃ³n retorna los resultados obtenidos.\n",
    "\n",
    "## Resumen\n",
    "Este cÃ³digo sigue un enfoque modular para:\n",
    "- **Optimizar el uso de memoria:** Al procesar el archivo JSON en pequeÃ±os fragmentos.\n",
    "- **Acelerar el procesamiento:** utiliza concurrencia para acelerar el procesamiento de grandes volÃºmenes de datos en un archivo JSON.\n",
    "- **Obtener resultados claros:** Retornando una lista donde cada tupla contiene el nombre del usuario y el conteo de las menciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2261),\n",
      " ('Kisanektamorcha', 1836),\n",
      " ('RakeshTikaitBKU', 1639),\n",
      " ('PMOIndia', 1422),\n",
      " ('RahulGandhi', 1125),\n",
      " ('GretaThunberg', 1046),\n",
      " ('RaviSinghKA', 1015),\n",
      " ('rihanna', 972),\n",
      " ('UNHumanRights', 962),\n",
      " ('meenaharris', 925)]\n",
      "Filename: C:\\Users\\Monica Zorrilla\\Documents\\AdaInformaticsLab\\latam-challenge\\src\\q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    15    267.6 MiB    267.6 MiB           1   @profile\n",
      "    16                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    17                                             \"\"\"\n",
      "    18                                             CALL: q3_time(file_path: str)\n",
      "    19                                             DESCRIPTION: Processes a JSON file to extract the top 10 mentioned users (Focus on optimizing time).\n",
      "    20                                             RESULT: List[Tuple[str, int]\n",
      "    21                                             \"\"\"\n",
      "    22    267.6 MiB      0.0 MiB           1       num_workers = multiprocessing.cpu_count()\n",
      "    23    267.6 MiB      0.0 MiB           1       analyzer = UserThreadAnalyzer(file_path, MEDIUM_CHUNK_SIZE, num_workers)\n",
      "    24    313.9 MiB     46.4 MiB           1       results = analyzer.analyze()\n",
      "    25                                             \n",
      "    26    313.7 MiB     -0.2 MiB           1       pprint(results, sort_dicts=False)\n",
      "    27    313.7 MiB      0.0 MiB           1       return results\n",
      "\n",
      "\n",
      "Total execution time: 9.728482 seconds\n"
     ]
    }
   ],
   "source": [
    "%run q3_time.py -file_path=\"large_files/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
